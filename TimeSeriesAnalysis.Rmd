---
title: "Time Series Analysis"
output: html_notebook
---

Exploring Time Series 

Date: Oct. 6, 2016

References: Datacamp

---------------------------

Start by looking at raw time series

```{r}
print(Nile)
```

Data about the water flow in river Nile from 1871 to 1970

```{r}
length(Nile)
```

```{r}
head(Nile)
```

displaying first few values using head

```{r}
head(Nile, n = 10)
```

Displaying the last few values using the tail command

```{r}
tail(Nile)
```

```{r}
plot(Nile, xlab = 'Year', ylab = 'River Volume (1e9 m^{3})', main = 'Annual River Nile Volume at Aswan, 1871-1970')
```

Plotting both points and the line

```{r}
plot(Nile, xlab = 'Year', ylab = 'River Volume (1e9 m^{3})', main = 'Annual River Nile Volume at Aswan, 1871-1970',  type = 'b')
# b stands for both: line and points
```

Next look at another data set AirPassengers: The classic Box & Jenkins airline data. Monthly totals of international airline passengers, 1949 to 1960.

```{r}
print(AirPassengers)
```


Exploring some of the built-in commands on the AirPassengers data set

"The start() and end() functions return the time index of the first and last observations. 
The time() function calculates a vector of time indices, with one element for each time index on which the series was observed.
The frequency() function returns the number of observations per unit time and the deltat() function returns the fixed time interval between observations. 
The cycle() function returns the position in the cycle of each observation."

```{r}
start(AirPassengers)

end(AirPassengers)
```

```{r}
time(AirPassengers)
```
```{r}
frequency(AirPassengers)
```

```{r}
deltat(AirPassengers)
```

Basically deltat = 1/ frequency

```{r}
cycle(AirPassengers)
```

```{r}
mean(AirPassengers)
```

```{r}
A <- c (1, 2, 3)
A
```

Let's explore the ts command

Construct a sample vector and convert it into ts object
The vector can be created using the c command
```{r}
sample_vector <- c(2.0521941073,  4.2928852797,  3.3294132944,  3.5085950670,  0.0009576938,
   1.9217186345,  0.7978134128,  0.2999543435,  0.9435687536,  0.5748283388,
 -0.0034005903,  0.3448649176,  2.2229761136,  0.1763144576,  2.7097622770,
  1.2501948965, -0.4007164754,  0.8852732121, -1.5852420266, -2.2829278891,
 -2.5609531290, -3.1259963754, -2.8660295895, -1.7847009207, -1.8894912908,
 -2.7255351194, -2.1033141800, -0.0174256893, -0.3613204151, -2.9008403327,
 -3.2847440927, -2.8684594718, -1.9505074437, -4.8801892525, -3.2634605353,
 -1.6396062522, -3.3012575840, -2.6331245433, -1.7058354022, -2.2119825061,
 -0.5170595186,  0.0752508095, -0.8406994716, -1.4022683487, -0.1382114230,
 -1.4065954703, -2.3046941055,  1.5073891432,  0.7118679477, -1.1300519022)
```

```{r}
plot(sample_vector)
```

Now let's convert the sample_vector into a time series object. 

"The function ts() can be applied to create time series objects. A time series object is a vector (univariate) or matrix (multivariate) with additional attributes, including time indices for each observation, the sampling frequency and time increment between observations, and the cycle length for periodic data. Such objects are of the ts class, and represent data that has been observed at (approximately) equally spaced time points. 

The advantage of creating and working with time series objects of the ts class is that many methods are available for utilizing time series attributes, such as time index information. For example, calling plot() on a ts object will automatically generate a plot over time."

```{r}
time_series <- ts(sample_vector)
plot(time_series, type = 'p')
```


```{r}
length(sample_vector)
```

The default starting point is 1 and the default deltat is 1 as well. 

```{r}
time_series <- ts(sample_vector, start = 2000, frequency = 4)
plot(time_series)
```
```{r}
time(time_series)
```

```{r}
time_series <- ts(sample_vector, start = 2000, deltat = 4)
plot(time_series)
```
```{r}
time(time_series)
```
```{r}
time_series <- ts(sample_vector, start = 2000, deltat = 0.25)
plot(time_series)
```

```{r}
time(time_series)
```

```{r}
print(time_series)

print(sample_vector)
```

The neat is.ts() command
```{r}
is.ts(time_series)
is.ts(sample_vector)
is.ts(Nile)
is.ts(AirPassengers)
```

Exploring the EU stocks data

```{r}
print(EuStockMarkets)
```


```{r}
head(EuStockMarkets, n = 132)
```
```{r}
tail(EuStockMarkets)
```

```{r}
start(EuStockMarkets)
```

I believed the second index in the output of start/end is the (number of rows - 1) having the given time index. 

This is what the documentation says: 
start: the time of the first observation. Either a single number or a vector of two integers, which specify a natural time unit and a (1-based) number of samples into the time unit.
(so, counting numbers of spacings?)

```{r}
end(EuStockMarkets)
```


```{r}
frequency(EuStockMarkets)
```
```{r}
deltat(EuStockMarkets)
```

```{r}
time(EuStockMarkets)
```
```{r}
plot(EuStockMarkets)
```
```{r}
ts.plot(EuStockMarkets, col = 1:4, xlab = "Year", ylab = "Index Value", main = "Major European Stock Indices, 1991-1998")
legend("topleft", colnames(EuStockMarkets), lty = 1, col = 1:4)
```


***** White Noise *****

```{r}
list(order = c(0,0,0))
```

```{r}
white_noise <- arima.sim(model = list(order = c(0,0,0)), n = 100)
print(white_noise)
```

```{r}
is.ts(white_noise)
```
```{r}
ts.plot(white_noise)
```

```{r}
white_noise2 <- arima.sim(model = list(c(0,0,0)), n = 100, mean = 100, sd = 20)
ts.plot(white_noise2)
```

```{r}
white_noise11 <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)
white_noise22 <- arima.sim(model = list(c(0, 0, 0)), n = 100)
white_noise33 <- arima.sim(model = list(order2 = c(0, 0, 0)), n = 100)
```

```{r}
white_noise11 - white_noise33
```
```{r}
white_noisea <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)
white_noiseb <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)
white_noisea - white_noiseb
```

Moral of the exercise above: it's not necessary to call order = .. inside the list function.  Also each call of the function generates a different series.
Correction: The random walk simulation below shows that the order is needed.  Surprisingly it doesn't matter if it's called order2, orderA etc. But orde is not OK.  

An ARIMA(p, d, q) model has three parts, the auto regressive order p, the order of integration (or differencing) d, and the moving average order q. 
The ARIMA(0, 0, 0) model is the white noise model. 


*** Random Walk ** 

The random walk (RW) model is the cumulative sum (or integration) of a mean zero white noise (WN) series, such that the first difference series of a RW is a WN series.  

RW is an ARIMA(0, 1, 0) model.

```{r}
random_walk <- arima.sim(model = list(order = c(0, 1, 0)), n = 100)
ts.plot(random_walk)
```

```{r}
ts.plot(diff(random_walk))
```

Random walk with a drift

```{r}
rw_drift <- arima.sim(model = list(order = c(0, 1, 0)), n = 1000, mean = 0.1)
ts.plot(rw_drift)
```

```{r}
mean(diff(rw_drift))
```

Note: the mean doesn't match if the n is a small number (the result is not surprising).  Plotting the diff(rw_drift) is another great way of looking at it. 

Estimating the RW model

```{r}
rw_drift_diff <- diff(rw_drift)
ts.plot(rw_drift_diff)
```

```{r}
model_wn <- arima(rw_drift_diff, order = c(0, 0, 0))
model_wn
```

```{r}
model_wn$coef
```

```{r}
int_wn <- model_wn$coef
```

```{r}
ts.plot(rw_drift)
abline(0, int_wn)
```

*** Exploring Stationarity **

```{r}
white_noise <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)

random_walk <- cumsum(white_noise)

wn_drift <- arima.sim(model = list(order = c(0, 0, 0)), n = 100, mean = 0.4)

rw_drift <- cumsum(wn_drift)

plot.ts(cbind(white_noise, random_walk, wn_drift, rw_drift))
```

```{r}
EuStockMarkets[-1860,]
```

*** Exploring financial time series **

```{r}
eu_percentreturns <- (EuStockMarkets[-1,] / EuStockMarkets[-1860,] - 1) * 100

head(eu_percentreturns)
```

```{r}
tail(eu_percentreturns)
```

```{r}
colMeans(eu_percentreturns)
```

```{r}
apply(eu_percentreturns, MARGIN = 2, FUN = var)
```

MARGIN = 2 tells that we want to apply the function over the columns.  FUN stands for the function

```{r}
apply(eu_percentreturns, MARGIN = 2, FUN = sd)
```

Note that var = sd^2 holds

```{r}
par(mfrow = c(2,2))
apply(eu_percentreturns, MARGIN = 2, FUN = hist, main = " ", xlab = "Percentage Return")
```

```{r}
par(mfrow = c(2,2))
apply(eu_percentreturns, MARGIN = 2, FUN = qqnorm, main = "")
qqline(eu_percentreturns)
```

From histograms, we can see that the vast majority of returns are near zero, but some daily returns are greater than 5 percentage points in magnitude.  Similarly, we see a clear departure from normality, especially in the tails of the distributions, as evident in the normal quantile plots. 

```{r}
head(eu_percentreturns)
```

```{r}
head(eu_percentreturns[,1])
```

Make scatter plots and find correltaion among different stock indices

```{r}
DAX <- EuStockMarkets[, 1]
FTSE <- EuStockMarkets[, 4]

plot(DAX, FTSE)
```

```{r}
pairs(EuStockMarkets)
```

```{r}
logreturns <- diff(log(EuStockMarkets))

plot(logreturns)
```

```{r}
pairs(logreturns)
```

The normal distribution has elliptical contours of equal probability. Pairs of data drawn from the multivariate normal distribution form a roughly elliptically shaped point cloud.  We see that the logreturns can be approximated by normal distribution. 

```{r}
DAX_logreturns <- logreturns[, 1]
FTSE_logreturns <- logreturns[, 4]
```

Blank space => take all rows

```{r}
cov(DAX_logreturns, FTSE_logreturns)
```

```{r}
cov(logreturns)
```

```{r}
cor(DAX_logreturns, FTSE_logreturns)
```
```{r}
corMat <- cor(logreturns)
corMat
```

Note that the correlation values are always between -1 and 1

```{r}
corFlat <- corMat[-1]
corFlat
```

```{r}
corFlat[-100]
```
```{r}
corFlat[-(1:2)]
```

*** Autocorrelation **

```{r}
x = c(2.06554379,  1.29963803,  0.03357800, -0.34258065,  0.23256126,  0.46812008,
      4.34111562,  2.82007636,  2.90799984,  2.33495061,  1.15989954,  0.82008659,
      -0.24338927, -0.03355907, -1.53548216, -0.69363797, -1.41731648, -0.76623179,
      0.83536060,  0.04395345,  1.07447506,  1.50200360, -0.21238609,  0.32996521,
  -0.75033470, -0.10522038,  0.20471918, -0.17170595,  0.87181378,  1.47213721,
   0.84112591,  0.96430157,  0.66829027, -0.25752691,  0.08193916, -1.46057000,
  -1.26726830, -2.19329186, -2.21008902,  0.42338945, -1.01513893, -1.54446229,
  -0.72524036,  0.70352378, -0.36108456, -0.77422092, -0.50023603,  1.31369378,
   1.15621723,  0.68782375, -0.79475183,  0.32563325,  2.00955556,  1.70614293,
   0.99910640,  0.68932712,  0.65764259,  1.51403467,  0.85806413,  1.96951273,
   2.98268339,  3.01781322,  1.30009671,  0.71140225,  0.40782908, -0.53429804,
  -0.21147251,  1.72814428, -0.75541665, -1.34178777, -1.72317007, -2.78147841,
  -1.72572507, -3.49466071, -2.41789449, -0.13744248, -0.15805310, -0.27865357,
  -0.97493500, -1.52666608, -1.04093146, -1.26059748, -1.44067012, -1.23902633,
  -0.44668174,  1.12562870,  3.25518488,  1.13570549,  0.98992411,  0.38244269,
   2.71124649,  2.42216865,  1.78509981, -1.03092109, -1.06607323, -2.63465306,
  -2.66808169, -1.30411399, -1.04269885,  0.40215260, -0.48928251, -0.49381470,
 -1.08457733, -0.27456945, -1.84390881, -2.09907629, -1.88923578, -1.84534263,
 -0.33812159, -1.20911695, -0.50157701, -0.58298734, -1.66575871, -1.41327839,
 -2.55380296, -0.86895290, -2.16915012, -2.60202618, -2.05678159, -0.87654110,
  1.32919650,  1.07620974, -0.96432698, -1.81480027, -2.05757608, -2.34353353,
 -0.01467163,  0.77321454,  0.03106214,  1.16999559,  2.67732293,  4.57761736,
  4.90582958,  4.13300371,  4.04398099,  1.35081333,  0.61429043,  1.42969023,
  0.79231154,  1.34178061,  2.22016551,  2.82502290,  2.43279283,  1.89023418,
  0.46877402, -1.30680558, -1.45910588,  0.21169330,  1.10203354,  1.42360646)

x_ts = ts(x)
```

```{r}
x_ts
```

```{r}
ts.plot(x_ts)
```


```{r}
n = length(x)
x_t0 <- x[-1]
x_t1 <- x[-n]
```

```{r}
head(cbind(x_t0, x_t1))
```

Confirming above that x_t0 and x_t1 are (x[t], x[t-1]) pairs

```{r}
plot(x_t0, x_t1)
```

```{r}
cor(x_t0, x_t1)
cor(x_t1, x_t0)
```

```{r}
acf(x_ts, lag.max = 1, plot = FALSE)
```

```{r}
cor(x_t0, x_t1) * ((n-1) / (n))
```

The two esimates are slightly different because they use slightly different scalings in the calculation of the calculation of the sample covariance. 

Now let's look at autocorrelation for different lag.max. 

```{r}
acf(x_ts, plot = FALSE)
```

```{r}
acf(x_ts, lag.max = 5, plot = FALSE)
```

```{r}
acf(x_ts, lag.max = 5, plot = FALSE)[3]
```

```{r}
acf(x_ts, lag.max = 10, plot = TRUE)
```

The pair of the dashed blue are called test bounds. Bounds test the null hypothesis that an autocorrelation coefficient is 0.  The null hypothesis is rejected if the sample correlation is outside the bound.  The usual level of the test is alpha = 0.05

** Autoregressive model **

Today's observation is regressed on yesterday's observation for all time t. 

Today = Constant + slope * yesterday + noise

Mean centered version:

(Today - mean) = slope (Yesterday - mean) + White noise (WN)

Y_t - mu = m (Y_{t-1} - mu) + epsilon_t

If m = 0, Y_t is a WN process

If m != 0, the Y_t depends on both epsilon_t and Y_{t-1} and the process is autocorrelated. 

Large values of m lead to greater autocorrelation (If m > 1, then so strong autocorrelation that the series quickly diverges).

Negative values of m lead to oscillatory time series. 

The versatile arima.sim() function used in previous chapters can also be used to simulate data from an AR model by setting the model argument equal to list(ar = phi) , in which phi is a slope parameter from the interval (-1, 1) . 

```{r}
# Simulate an AR model with 0.5 slope
x <- arima.sim(model = list(ar = 0.5), n = 100)
```

```{r}
# Simulate an AR model with slope greater than 1
x <- arima.sim(model = list(ar = 1.5), n = 100)
```
```{r}
# Simulate an AR model with 0.9 slope
y <- arima.sim(model = list(ar = 0.9), n = 100)

# Simulate an AR model with -0.75 slope
z <- arima.sim(model = list(ar = -0.75), n = 100)
```

Plot simulated data

```{r}
plot(cbind(x, y, z), type = 'b')
```

```{r}
plot.ts(cbind(x, y, z), type = 'b')
```

Autocorrelation of autogression processes. 

```{r}
acf(x)
```

```{r}
acf(y)
```

```{r}
acf(z)
```

Note that autocorrelation follows the expectation.  When the magnitude of the slope in the autoregressor model is large the autocorrelation is large (and persists for larger time lags). When the magnitude of the slope is small, the autocorrelation quickly decays beyond the first few time lags.  When the slope is negative, the autocorrelation oscillates sign. 

```{r}
sample_vec <- c(rep(1,50))
sample_vec
```

```{r}
sample_ts <- ts(sample_vec)
sample_ts
```

```{r}
cov(sample_ts[-50], sample_ts[-1])
```

```{r}
white_noise <- arima.sim(model = list(order = c(0, 0, 0)), n = 100)
acf(white_noise)
```

Random walk (RW) and autoregressive (AR) models 

```{r}
x <- arima.sim(model = list(ar = 0.9), n = 200)
ts.plot(x)
```
```{r}
acf(x)
```

```{r}
y <- arima.sim(model = list(ar = 0.98), n = 200)
ts.plot(y)
acf(y)
```

```{r}
z <- arima.sim(model = list(order = c(0, 1, 0)), n = 200)
ts.plot(z)
```

```{r}
acf(z)
```

The random walk (RW) model is a special case of the autoregressive (AR) model, in which the slope parameter is equal to 1. Recall that the RW model is not stationary and exhibits very strong persistence. Its sample autocovariance function (ACF) also decays to zero very slowly, meaning past values have a long lasting impact on current values.

The stationary AR model has a slope parameter between -1 and 1. The AR model exhibits higher persistence when its slope parameter is closer to 1, but the process reverts to its mean fairly quickly (compared to RW). Its sample ACF also decays to zero at a geometric rate, indicating that values far in the past have little impact on future values of the process.